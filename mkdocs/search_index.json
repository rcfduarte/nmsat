{
    "docs": [
        {
            "location": "/", 
            "text": "Neural Microcircuit Simulation and Analysis Toolkit (NMSAT)\n\n\nWhat is it?\n\n\nNMSAT is a python package that provides a set of tools to build, simulate and analyse neuronal\nmicrocircuit models with any degree of complexity, as well as to probe the circuits with arbitrarily\ncomplex input stimuli / signals and to analyse the relevant functional aspects of single neuron,\npopulation and network dynamics. It provides a high-level wrapper for PyNEST (which is used as\nthe core simulation engine). As such, the complexity of the microcircuits analysed and their building\nblocks (neuron and synapse models, circuit topology and connectivity, etc.), are determined by the\nmodels available in NEST. The use of NEST allows efficient and highly scalable simulations of very\nlarge and complex circuits, constrained only by the computational resources available to the user.\nThe modular design allows the user to specify numerical experiments with varying degrees of\ncomplexity depending on concrete research objectives. The generality of some of these experiments\nallows the same types of measurements to be performed on a variety of different circuits, which can\nbe useful for benchmarking and comparison purposes. Additionally, the code was designed to allow\nan effortless migration across computing systems, i.e. the same simulations can be executed in a\nlocal machine, in a computer cluster or a supercomputer, with straightforward resource allocation\n(see kernel parameters).\n\n\nThe code is licensed under GPLv3 and available on \nGitHub\n.\n\n\nDisclaimer\n\n\nThe code was developed primarily for personal use, as part of a PhD thesis due to the need to perform\nsimilar experiments and analyses on very diverse systems. The goal was to use the same code to\nrun diverse numerical experiments, covering a broad range of complexity, in a fully transparent and\nreproducible manner, while making efficient use of computing resources. Due to the inherent time\npressures of a PhD project and the very broad scope, the code is imperfect and under active use /\ndevelopment. Despite our best efforts, it is prone to errors and often difficult to understand, particularly\ndue to the strict specificities on the structure of the parameters dictionaries and how they are used.\n\n\nAbout the authors\n\n\nRenato Duarte ...", 
            "title": "Overview"
        }, 
        {
            "location": "/#neural-microcircuit-simulation-and-analysis-toolkit-nmsat", 
            "text": "", 
            "title": "Neural Microcircuit Simulation and Analysis Toolkit (NMSAT)"
        }, 
        {
            "location": "/#what-is-it", 
            "text": "NMSAT is a python package that provides a set of tools to build, simulate and analyse neuronal\nmicrocircuit models with any degree of complexity, as well as to probe the circuits with arbitrarily\ncomplex input stimuli / signals and to analyse the relevant functional aspects of single neuron,\npopulation and network dynamics. It provides a high-level wrapper for PyNEST (which is used as\nthe core simulation engine). As such, the complexity of the microcircuits analysed and their building\nblocks (neuron and synapse models, circuit topology and connectivity, etc.), are determined by the\nmodels available in NEST. The use of NEST allows efficient and highly scalable simulations of very\nlarge and complex circuits, constrained only by the computational resources available to the user.\nThe modular design allows the user to specify numerical experiments with varying degrees of\ncomplexity depending on concrete research objectives. The generality of some of these experiments\nallows the same types of measurements to be performed on a variety of different circuits, which can\nbe useful for benchmarking and comparison purposes. Additionally, the code was designed to allow\nan effortless migration across computing systems, i.e. the same simulations can be executed in a\nlocal machine, in a computer cluster or a supercomputer, with straightforward resource allocation\n(see kernel parameters).  The code is licensed under GPLv3 and available on  GitHub .", 
            "title": "What is it?"
        }, 
        {
            "location": "/#disclaimer", 
            "text": "The code was developed primarily for personal use, as part of a PhD thesis due to the need to perform\nsimilar experiments and analyses on very diverse systems. The goal was to use the same code to\nrun diverse numerical experiments, covering a broad range of complexity, in a fully transparent and\nreproducible manner, while making efficient use of computing resources. Due to the inherent time\npressures of a PhD project and the very broad scope, the code is imperfect and under active use /\ndevelopment. Despite our best efforts, it is prone to errors and often difficult to understand, particularly\ndue to the strict specificities on the structure of the parameters dictionaries and how they are used.", 
            "title": "Disclaimer"
        }, 
        {
            "location": "/#about-the-authors", 
            "text": "Renato Duarte ...", 
            "title": "About the authors"
        }, 
        {
            "location": "/installation/", 
            "text": "Download\n\n\nWe currently don\u2019t provide the code as a standard library that can be easily installed. In future\nversions, we will try to work on this and improve usability. For the moment, to use the code there is\nno explicit installation involved. Just fork and clone the github repository from \nhere.\n\n\nDependencies\n\n\n\n\nPython 2.7\n\n\nNEST\n version 2.8.0 or higher\n\n\nnumpy version 1.7.0 or higher \n\n\nscipy version 0.12.0 or higher\n\n\nscikit-learn version 0.18.0 or higher\n\n\nmatplotlib version 1.2.0 or higher\n\n\n\n\nOptional (add functionality):\n\n\n\n\nPySpike\n version 0.5.1\n\n\nh5py version 2.2.1 or higher\n\n\nmayavi \n\n\n\n\nGetting Started\n\n\nAfter downloading the repository, the framework's configure file, which sets the correct paths, needs to be sourced. \nThe following line should be added to .bashrc (*):\n\n\nsource\n /\n{\npath\n}\n/nmsat/configure.sh\n\n\n\n\n\nwhere \n{path}\n refers to the full path to the main NMSAT directory in your system.\nThe last step requires the user to manually specify all the paths for his system, by editing the paths dictionary in \n/defaults/paths.py\n, as:\n\n\npaths\n \n=\n \n{\n\n    \nsystem_label\n:\n \n{\n\n        \ndata_path\n:\n            \nNMSAT_HOME\n \n+\n \n/data/\n,\n\n        \njdf_template\n:\n         \nNMSAT_HOME\n \n+\n \n/defaults/cluster_templates/Blaustein_jdf.sh\n,\n\n        \nmatplotlib_rc\n:\n        \nNMSAT_HOME\n \n+\n \n/defaults/matplotlib_rc\n,\n\n        \nremote_directory\n:\n     \nNMSAT_HOME\n \n+\n \n/export/\n,\n\n        \nqueueing_system\n:\n      \nslurm\n}\n\n        \n}\n\n\n\n\n\n\nThe \nsystem_label\n specifies the name of the system. If running simulations on a local machine, the name must be set as 'local' (which is the default), otherwise, it can be any arbitrary name, as long as it is used consistently throughout (see examples). The remaining entries in this dictionary refer to:\n\n\n\n\ndata_path\n - specify where to store the output data generated by an experiment\n\n\njdf_template\n - path to a system-specific job description file (see example in /defaults/cluster_templates); if running locally set to None\n\n\nmatplotlibrc\n - in case the user wants to customize matplotlib\\footnote{\\url{\nhttp://matplotlib.org/users/customizing.html\n}}\n\n\nremote_directory\n - folder where the job submission files will be written (only applicable if not running locally, but must be specified anyway)\n\n\nqueueing_system\n - type of job schedulling system used (current options include 'slurm' and 'sge')..\n\n\n\n\nAnd that should be it. After specifying all these, it should be possible to run the framework (see below).", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/#download", 
            "text": "We currently don\u2019t provide the code as a standard library that can be easily installed. In future\nversions, we will try to work on this and improve usability. For the moment, to use the code there is\nno explicit installation involved. Just fork and clone the github repository from  here.", 
            "title": "Download"
        }, 
        {
            "location": "/installation/#dependencies", 
            "text": "Python 2.7  NEST  version 2.8.0 or higher  numpy version 1.7.0 or higher   scipy version 0.12.0 or higher  scikit-learn version 0.18.0 or higher  matplotlib version 1.2.0 or higher   Optional (add functionality):   PySpike  version 0.5.1  h5py version 2.2.1 or higher  mayavi", 
            "title": "Dependencies"
        }, 
        {
            "location": "/installation/#getting-started", 
            "text": "After downloading the repository, the framework's configure file, which sets the correct paths, needs to be sourced. \nThe following line should be added to .bashrc (*):  source  / { path } /nmsat/configure.sh  where  {path}  refers to the full path to the main NMSAT directory in your system.\nThe last step requires the user to manually specify all the paths for his system, by editing the paths dictionary in  /defaults/paths.py , as:  paths   =   { \n     system_label :   { \n         data_path :              NMSAT_HOME   +   /data/ , \n         jdf_template :           NMSAT_HOME   +   /defaults/cluster_templates/Blaustein_jdf.sh , \n         matplotlib_rc :          NMSAT_HOME   +   /defaults/matplotlib_rc , \n         remote_directory :       NMSAT_HOME   +   /export/ , \n         queueing_system :        slurm } \n         }   The  system_label  specifies the name of the system. If running simulations on a local machine, the name must be set as 'local' (which is the default), otherwise, it can be any arbitrary name, as long as it is used consistently throughout (see examples). The remaining entries in this dictionary refer to:   data_path  - specify where to store the output data generated by an experiment  jdf_template  - path to a system-specific job description file (see example in /defaults/cluster_templates); if running locally set to None  matplotlibrc  - in case the user wants to customize matplotlib\\footnote{\\url{ http://matplotlib.org/users/customizing.html }}  remote_directory  - folder where the job submission files will be written (only applicable if not running locally, but must be specified anyway)  queueing_system  - type of job schedulling system used (current options include 'slurm' and 'sge')..   And that should be it. After specifying all these, it should be possible to run the framework (see below).", 
            "title": "Getting Started"
        }, 
        {
            "location": "/framework-description/", 
            "text": "The code is organized as follows:\n\n\n\u251c\u2500\u2500 nmsat\n\u2502   \u251c\u2500\u2500 modules\n\u2502   \u2502   \u251c\u2500\u2500 parameters.py\n\u2502   \u2502   \u251c\u2500\u2500 input_architect.py\n\u2502   \u2502   \u251c\u2500\u2500 net_architect.py\n\u2502   \u2502   \u251c\u2500\u2500 signals.py\n\u2502   \u2502   \u251c\u2500\u2500 analysis.py\n\u2502   \u2502   \u251c\u2500\u2500 visualization.py\n\u2502   \u2502   \u251c\u2500\u2500 io.py\n\u2502   \u251c\u2500\u2500 defaults\n\u2502   \u2502   \u251c\u2500\u2500 paths.py\n\u2502   \u2502   \u251c\u2500\u2500 matplotlib_rc\n\u2502   \u2502   \u251c\u2500\u2500 cluster_templates\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 cluster_jdf.sh\n\u2502   \u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 projects\n\u2502   \u2502   \u251c\u2500\u2500 project_name\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 computations\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 parameters\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 preset\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 scripts\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 read_data\n\u2502   \u251c\u2500\u2500 export\n\n\n\n\n\nThe core functionality lies in the modules packages, which contain all the relevant classes and\nfunctions used. The specifics will be explained in greater detail below, but in general the modules\nare responsible for:\n\n\n\n\nparameters\n - parsing and preparing all parameters files; retrieving stored parameter sets and\nspaces and harvesting data\n\n\ninput_architect\n - generating and setting up all the relevant input stimuli and signals; handling\ninput data; generating and connecting input encoding layers\n\n\nnet_architect\n - generating specific networks and neuronal populations; generating all connectivity and topology features; \nconnecting populations; ...\n\n\nsignals\n - wrapping and processing the various signal types used in the framework (spiking activity,\nanalog variables, etc)\n\n\nanalysis\n - post-processing and analysing population activity in various ways\n\n\nvisualization\n - plotting routines\n\n\nio\n - loading and saving data", 
            "title": "Quick overview"
        }, 
        {
            "location": "/standard-use-case/", 
            "text": "A numerical experiment in this framework consists of 2 or 3 main files (see examples):\n\n\n\n\nparameters_file\n - specifying all the complex parameter sets and dictionaries required to set up\nthe experiment.\n\n\nexperiment_script\n - mostly used during development, for testing and debugging purposes.\nThese scripts parse the parameters_file and run the complete experiment\n\n\ncomputation_file\n - after the development and testing phase is completed, the experiment can\nbe copied to a computation_file, which can be used from the main... (*)\n\n\n\n\nThese files should be stored within a \"project\" folder, in the \"parameters\", \"scripts\" and \"compu-\ntations\" folders, respectively.\n\n\nRunning an experiment\n\n\nThe way in which an experiment is run depends on the system used:\n\n\nLocal machine\n\n\nJust go to the main nmsat directory and execute the experiment as:\n\n\npython\n \nmain\n.\npy\n \n-\nf\n \n{\nparameters_file\n}\n \n-\nc\n \n{\ncomputation_function\n}\n \n--\nextra\n \n{\nextra_parameters\n}\n\n\n\n\n\n\nwhere \nparameters_file\n refers to the (full or relative) path to the parameters file for the experiment,\n\ncomputation_function\n is the name of the computation function to be executed on that parameter\nset (must match the name of a file in the project\u2019s \u2019computations\u2019 folder) and \nextra_parameters\n are\nparameter=value pairs for different, extra parameters (specific to each computation).\n\n\nCluster\n\n\nOn a computer cluster or supercomputer, the execution of the framework has a slightly different meaning. \nInstead of executing the code, it generates a series of files that can be used to submit the jobs to the system\u2019s scheduler.\n\n\nTo do this:\n\n\n\n\nadd an entry to \nnmsat/defaults/paths.py\n for your template (here \u2019Blaustein\u2019)\n\n\nadapt the default cluster template in \nnmsat/defaults/cluster_templates/Blaustein_jdf.sh\n\nto match your cluster requirements\n\n\nchange \nrun=\u2019local\u2019\n to \nrun=\u2019Blaustein\u2019\n in your parameter script\n\n\nexecute the following command from \nnmsat/\n\n\n\n\npython\n \nmain\n.\npy\n \n-\nf\n \n{\nparameters_file\n}\n \n-\nc\n \n{\ncomputation_function\n}\n \n--\nextra\n \n{\nextra_parameters\n}\n \n--\ncluster\n=\nBlaustein\n\n\n\n\n\n\n\n\ngo to \nnmsat/export/my_project_name/\n and submit jobs  via\n\n\n\n\npython\n \nsubmit_jobs\n.\npy\n \n0\n \n1\n\n\n\n\n\n\nSimulation output\n\n\nAfter a simulation is completed, all the relevant output data is stored in the pre-specified data_path,\nwithin a folder named after the project label. The output data structure is organized as follows:\n\n\ndata\n\u251c\u2500\u2500 experiment_label\n\u2502   \u251c\u2500\u2500 Figures\n\u2502   \u251c\u2500\u2500 Results\n\u2502   \u251c\u2500\u2500 Parameters\n\u2502   \u251c\u2500\u2500 Activity\n\u2502   \u251c\u2500\u2500 Input\n\u2502   \u251c\u2500\u2500 Output\n\n\n\n\n\nAnalysing and plotting\n\n\nAnalysis and plotting can be (and usually is) done within the main computation, so as to extract and\nstore only the information that is relevant for the specific experiment. Multiple, standard analysis and\nplotting routines are implemented for various complex experiments, with specific objectives. Naturally,\nthis is highly mutable as new experiments always require specific analyses methods.\n\n\nAlternatively, as all the relevant data is stored in the results dictionaries, you can read it and\nprocess it offline, applying the same or novel visualization routines.\n\n\nHarvesting stored results\n\n\nThe Results folder stores all the simulation results for the given experiment, as pickle dictionaries.\nWithin each project, as mentioned earlier, a read_data folder should be included, which contains files\nto parse and extract the stored results (see examples).\n\n\nproject\n      \n=\n \nproject_name\n\n\ndata_path\n    \n=\n \n/path/label/\n\n\ndata_label\n   \n=\n \nexample1\n\n\nresults_path\n \n=\n \ndata_path\n \n+\n \ndata_label\n \n+\n \n/Results/\n\n\n\n# set defaults and paths\n\n\nset_project_paths\n(\nproject\n)\n\n\nset_global_rcParams\n(\npaths\n[\nlocal\n][\nmatplotlib_rc\n])\n\n\n\n# re-create ParameterSpace\n\n\npars_file\n \n=\n \ndata_path\n \n+\n \ndata_label\n \n+\n \n_ParameterSpace.py\n\n\npars\n \n=\n \nParameterSpace\n(\npars_file\n)\n\n\n\n# print the full nested structure of the results dictionaries\n\n\npars\n.\nprint_stored_keys\n(\nresults_path\n)\n\n\n\n# harvest a specific result based on the results structure\n\n\ndata_array\n \n=\n \npars\n.\nharvest\n(\nresults_path\n,\n \nkey_set\n=\ndict_key1/dict_key1.1/dict_key1.1.1\n)", 
            "title": "Standard use case"
        }, 
        {
            "location": "/standard-use-case/#running-an-experiment", 
            "text": "The way in which an experiment is run depends on the system used:", 
            "title": "Running an experiment"
        }, 
        {
            "location": "/standard-use-case/#local-machine", 
            "text": "Just go to the main nmsat directory and execute the experiment as:  python   main . py   - f   { parameters_file }   - c   { computation_function }   -- extra   { extra_parameters }   where  parameters_file  refers to the (full or relative) path to the parameters file for the experiment, computation_function  is the name of the computation function to be executed on that parameter\nset (must match the name of a file in the project\u2019s \u2019computations\u2019 folder) and  extra_parameters  are\nparameter=value pairs for different, extra parameters (specific to each computation).", 
            "title": "Local machine"
        }, 
        {
            "location": "/standard-use-case/#cluster", 
            "text": "On a computer cluster or supercomputer, the execution of the framework has a slightly different meaning. \nInstead of executing the code, it generates a series of files that can be used to submit the jobs to the system\u2019s scheduler.  To do this:   add an entry to  nmsat/defaults/paths.py  for your template (here \u2019Blaustein\u2019)  adapt the default cluster template in  nmsat/defaults/cluster_templates/Blaustein_jdf.sh \nto match your cluster requirements  change  run=\u2019local\u2019  to  run=\u2019Blaustein\u2019  in your parameter script  execute the following command from  nmsat/   python   main . py   - f   { parameters_file }   - c   { computation_function }   -- extra   { extra_parameters }   -- cluster = Blaustein    go to  nmsat/export/my_project_name/  and submit jobs  via   python   submit_jobs . py   0   1", 
            "title": "Cluster"
        }, 
        {
            "location": "/standard-use-case/#simulation-output", 
            "text": "After a simulation is completed, all the relevant output data is stored in the pre-specified data_path,\nwithin a folder named after the project label. The output data structure is organized as follows:  data\n\u251c\u2500\u2500 experiment_label\n\u2502   \u251c\u2500\u2500 Figures\n\u2502   \u251c\u2500\u2500 Results\n\u2502   \u251c\u2500\u2500 Parameters\n\u2502   \u251c\u2500\u2500 Activity\n\u2502   \u251c\u2500\u2500 Input\n\u2502   \u251c\u2500\u2500 Output", 
            "title": "Simulation output"
        }, 
        {
            "location": "/standard-use-case/#analysing-and-plotting", 
            "text": "Analysis and plotting can be (and usually is) done within the main computation, so as to extract and\nstore only the information that is relevant for the specific experiment. Multiple, standard analysis and\nplotting routines are implemented for various complex experiments, with specific objectives. Naturally,\nthis is highly mutable as new experiments always require specific analyses methods.  Alternatively, as all the relevant data is stored in the results dictionaries, you can read it and\nprocess it offline, applying the same or novel visualization routines.", 
            "title": "Analysing and plotting"
        }, 
        {
            "location": "/standard-use-case/#harvesting-stored-results", 
            "text": "The Results folder stores all the simulation results for the given experiment, as pickle dictionaries.\nWithin each project, as mentioned earlier, a read_data folder should be included, which contains files\nto parse and extract the stored results (see examples).  project        =   project_name  data_path      =   /path/label/  data_label     =   example1  results_path   =   data_path   +   data_label   +   /Results/  # set defaults and paths  set_project_paths ( project )  set_global_rcParams ( paths [ local ][ matplotlib_rc ])  # re-create ParameterSpace  pars_file   =   data_path   +   data_label   +   _ParameterSpace.py  pars   =   ParameterSpace ( pars_file )  # print the full nested structure of the results dictionaries  pars . print_stored_keys ( results_path )  # harvest a specific result based on the results structure  data_array   =   pars . harvest ( results_path ,   key_set = dict_key1/dict_key1.1/dict_key1.1.1 )", 
            "title": "Harvesting stored results"
        }, 
        {
            "location": "/parameters/", 
            "text": "The implementation of the simulator relies entirely on the structure of the parameters dictionaries,\nwhich makes the correct specification of the parameters files the most sensitive (and error-prone)\naspect. The main modules simply extract the specifications described in these dictionaries and set-up\nthe experiment. So, the most critical step in setting up a numerical experiment with the current\nimplementation is the correct specification of all the complex parameter sets. In this section, we\nexplain the main features that all parameters must obey to and exemplify how the parameters file\ncontaining all the dictionaries should be set up. However, different experiments can have very different\nrequirements and specifications (see examples). It is important to reinforce that the structure of these\ndictionaries is the critical element to make everything work. If the naming conventions used are broken,\nerrors will follow.\n\n\nFurthermore, modifications of the parameter values are accepted with some restrictions, depending\non what is currently implemented.\n\n\nTypically the following imports are necessary in a parameter file:\n\n\nfrom\n \ndefaults.paths\n \nimport\n \npaths\n  \n\nfrom\n \nmodules.parameters\n \nimport\n \nParameterSet\n\n\n\n\n\n\nThe experiment label and the system in which the experiments will run need to be specified and\nwill determine the system-specific paths to use as well as the label for data storage:\n\n\nsystem_name\n \n=\n \nlocal\n\n\ndata_label\n  \n=\n \nexample1_singleneuron_fI\n\n\n\n\n\n\nIt is important to make sure that the system_name corresponds to a key in the paths dictionary.\nAlso, it is always advisable to provide apropriate labels for the data.\n\n\nParameter range declarations and build function\n\n\n(...)\n\n\nreturn\n \ndict\n([(\nkernel_pars\n,\n \nkernel_pars\n),\n\n             \n(\nneuron_pars\n,\n \nneuron_pars\n),\n\n             \n(\nnet_pars\n,\n \nnet_pars\n),\n\n             \n(\nencoding_pars\n,\n \nencoding_pars\n)])\n\n\n\n\n\n\nParameter types\n\n\nThe output of the build_parameters function is a dictionary of ParameterSets, containing all the\nnecessary types of parameters to be used by the main experiment. Acceptable types (with examples\nof use) are:\n\n\nKernel\n\n\nSpecify all relevant system and simulation parameters.\n\n\nkernel_pars\n \n=\n \nParameterSet\n({\n\n    \nresolution\n:\n \n0.1\n,\n      \n# simulation resolution\n\n    \nsim_time\n:\n \n1000.\n,\n      \n# total simulation time (often not required)\n\n    \ntransient_t\n:\n \n0.\n,\n      \n# transient time \n\n    \ndata_prefix\n:\n \ndata_label\n,\n\n    \ndata_path\n:\n \npaths\n[\nsystem_name\n][\ndata_path\n],\n\n    \nmpl_path\n:\n \npaths\n[\nsystem_name\n][\nmatplotlib_rc\n],\n\n    \noverwrite_files\n:\n \nTrue\n,\n\n    \nprint_time\n:\n \n(\nsystem_name\n \n==\n \nlocal\n),\n\n    \nrng_seeds\n:\n \nrange\n(\nmsd\n \n+\n \nN_vp\n \n+\n \n1\n,\n \nmsd\n \n+\n \n2\n \n*\n \nN_vp\n \n+\n \n1\n),\n\n    \ngrng_seed\n:\n \nmsd\n \n+\n \nN_vp\n,\n\n    \ntotal_num_virtual_procs\n:\n \nN_vp\n,\n\n    \nlocal_num_threads\n:\n \n16\n,\n\n    \nnp_seed\n:\n \nnp_seed\n,\n\n\n    \nsystem\n:\n \n{\n\n        \nlocal\n:\n \n(\nsystem_name\n \n==\n \nlocal\n),\n\n        \nsystem_label\n:\n \nsystem_name\n,\n\n        \nqueueing_system\n:\n \npaths\n[\nsystem_name\n][\nqueueing_system\n],\n\n        \njdf_template\n:\n \npaths\n[\nsystem_name\n][\njdf_template\n],\n\n        \nremote_directory\n:\n \npaths\n[\nsystem_name\n][\nremote_directory\n],\n\n        \njdf_fields\n:\n \n{\n{{ script_folder }}\n:\n \n,\n\n                       \n{{ nodes }}\n:\n \nstr\n(\nsystem_pars\n[\nnodes\n]),\n\n                       \n{{ ppn }}\n:\n \nstr\n(\nsystem_pars\n[\nppn\n]),\n\n                       \n{{ mem }}\n:\n \nstr\n(\nsystem_pars\n[\nmem\n]),\n\n                       \n{{ walltime }}\n:\n \nsystem_pars\n[\nwalltime\n],\n\n                       \n{{ queue }}\n:\n \nsystem_pars\n[\nqueue\n],\n\n                       \n{{ computation_script }}\n:\n \n}}})\n\n\n\n\n\n\nIf correctly specified, these parameters can be used during the initial setup of the main simulations:\n\n\nset_global_rcParams\n(\nparameter_set\n.\nkernel_pars\n[\nmpl_path\n])\n\n\nnp\n.\nrandom\n.\nseed\n(\nparameter_set\n.\nkernel_pars\n[\nnp_seed\n])\n\n\nnest\n.\nSetKernelStatus\n(\nextract_nestvalid_dict\n(\nkernel_pars\n.\nas_dict\n(),\n \nparam_type\n=\nkernel\n))\n\n\n\n\n\n\nNeuron\n\n\nneuron model parameters, must stricly abide by the naming conventions of the NEST\nmodel requested. For example:\n\n\nneuron_pars\n \n=\n \n{\n\n    \nneuron_name\n:\n \n{\n\n        \nmodel\n:\n \niaf_psc_exp\n,\n\n        \nC_m\n:\n \n250.0\n,\n\n        \nE_L\n:\n \n0.0\n,\n\n        \nV_reset\n:\n \n0.0\n,\n\n        \nV_th\n:\n \n15.\n,\n\n        \nt_ref\n:\n \n2.\n,\n\n        \ntau_syn_ex\n:\n \n2.\n,\n\n        \ntau_syn_in\n:\n \n2.\n,\n\n        \ntau_m\n:\n \n20.\n}}\n\n\n\n\n\n\nTypically the neuron parameters are only used within the parameters file, as they will be placed\nin the network parameter dictionary, where the code will use them.\n\n\nNetwork\n\n\nNetwork parameters, specifying the composition, topology and which variables to record\nfrom the multiple populations:\n\n\nnet_pars\n \n=\n \n{\n\n    \nn_populations\n:\n \n2\n,\n         \n# total number of populations\n\n    \npop_names\n:\n \n[\nE\n,\n \nI\n],\n\n    \nn_neurons\n:\n \n[\n8000\n,\n \n2000\n],\n\n    \nneuron_pars\n:\n \n[\nneuron_pars\n[\nE\n],\n \nneuron_pars\n[\nI\n]],\n\n    \nrandomize_neuron_pars\n:\n \n[{\nV_m\n:\n \n(\nnp\n.\nrandom\n.\nuniform\n,\n \n{\nlow\n:\n \n0.0\n,\n \nhigh\n:\n \n15.\n})},\n \n# if necessary\n\n                              \n{\nV_m\n:\n \n(\nnp\n.\nrandom\n.\nuniform\n,\n \n{\nlow\n:\n \n0.0\n,\n \nhigh\n:\n \n15.\n})}],\n\n    \ntopology\n:\n \n[\nFalse\n,\n \nFalse\n],\n\n    \ntopology_dict\n:\n \n[\nNone\n,\n \nNone\n],\n\n    \nrecord_spikes\n:\n \n[\nTrue\n,\n \nTrue\n],\n\n    \nspike_device_pars\n:\n \n[\ncopy_dict\n(\nrec_devices\n,\n\n                                    \n{\nmodel\n:\n \nspike_detector\n,\n\n                                     \nrecord_to\n:\n \n[\nmemory\n],\n\n                                     \nlabel\n:\n \n}),\n\n                          \ncopy_dict\n(\nrec_devices\n,\n\n                                    \n{\nmodel\n:\n \nspike_detector\n,\n\n                                     \nrecord_to\n:\n \n[\nmemory\n],\n\n                                     \nlabel\n:\n \n})],\n\n    \nrecord_analogs\n:\n \n[\nFalse\n,\n \nFalse\n],\n\n    \nanalog_device_pars\n:\n \n[\nNone\n,\n \nNone\n]}\n\n\n\n\n\n\nNote that the dimensionality of all the list parameters has to be equal to n_populations. If\nthese parameters are all correctly set, generating a network is as simple as:\n\n\nnet\n \n=\n \nNetwork\n(\nnet_pars\n)\n\n\nnet\n.\nconnect_devices\n()\n\n\n\n\n\n\nConnection\n\n\nConnectivity parameters, specifying the composition, topology and connectivity of\nthe multiple populations:\n\n\nconnection_pars\n \n=\n \n{\n\n    \nn_synapse_types\n:\n \n4\n,\n   \n# [int] - total number of connections to establish \n\n    \nsynapse_types\n:\n    \n# [list of tuples] - each tuple corresponds to 1 connection and \n\n                \n# has the form (tget_pop_label, src_pop_label)   \n\n        \n[(\nE\n,\n \nE\n),\n \n(\nE\n,\n \nI\n),\n \n(\nI\n,\n \nE\n),\n \n(\nI\n,\n \nI\n)],\n \n    \ntopology_dependent\n:\n   \n# [list of bool] - whether synaptic connections are \n\n                \n# established relying on nest.topology module\n\n        \n[\nFalse\n,\n \nFalse\n,\n \nFalse\n,\n \nFalse\n],\n \n    \nmodels\n:\n       \n# [list of str] - names of the synapse models to realize on each\n\n                \n# synapse type\n\n        \n[\nstatic_synapse\n,\n \nstatic_synapse\n,\n \nstatic_synapse\n,\n \nstatic_synapse\n],\n           \n    \nmodel_pars\n:\n       \n# [list of dict] - each entry in the list contains the synapse\n\n                \n# dictionary (if necessary) for the corresponding synapse model\n\n        \n[\nsynapse_pars_dict\n,\n \nsynapse_pars_dict\n,\n \nsynapse_pars_dict\n,\n \nsynapse_pars_dict\n],\n     \n    \nweight_dist\n:\n  \n# [list of dict] -  \n\n        \n[\ncommon_syn_specs\n[\nweight\n],\n \ncommon_syn_specs\n[\nweight\n],\n \n         \ncommon_syn_specs\n[\nweight\n],\n \ncommon_syn_specs\n[\nweight\n]],\n\n    \npre_computedW\n:\n \n# Provide an externally specified connectivity matrix\n\n            \n# if pre-computed weights matrices are provided, delays also\n\n            \n# need to be pre-specified, as an array with the same dimensions\n\n            \n# as W, with delay values on each corresponding entry.\n\n            \n# Alternatively, if all delays are the same, just provide a\n\n            \n# single number.\n\n        \n[\nNone\n,\n \nNone\n,\n \nNone\n,\n \nNone\n],\n\n\n    \ndelay_dist\n:\n \n[\ncommon_syn_specs\n[\ndelay\n],\n \ncommon_syn_specs\n[\ndelay\n],\n\n                \ncommon_syn_specs\n[\ndelay\n],\n \ncommon_syn_specs\n[\ndelay\n]],\n\n\n    \nconn_specs\n:\n   \n# [list of dict] - for topological connections\n\n        \n[\nconn_specs\n,\n \nconn_specs\n,\n \nconn_specs\n,\n \nconn_specs\n],\n\n\n    \nsyn_specs\n:\n \n[]\n\n\n\n\n\n\nTo connect the network, just provide this parameter set to the connect method of the Network\nobject (for complex connectivity schemes, it is worth setting the progress=True to accompany the\nconnection progress):\n\n\nnet\n.\nconnect_populations\n(\nparameter_set\n.\nconnection_pars\n,\n \nprogress\n=\nTrue\n)\n\n\n\n\n\n\nStimulus\n\n\nStimulus or task parameters:\n\n\nstim_pars\n \n=\n \n{\n\n        \nn_stim\n:\n \n10\n,\n       \n# number of stimuli\n\n        \nelements\n:\n \n[\nA\n,\n \nB\n,\n \nC\n,\n \n...\n],\n\n        \ngrammar\n:\n \nNone\n,\n \n#!!\n\n        \nfull_set_length\n:\n \nint\n(\nT\n \n+\n \nT_discard\n),\n \n# total samples\n\n        \ntransient_set_length\n:\n \nint\n(\nT_discard\n),\n \n        \ntrain_set_length\n:\n \nint\n(\n0.8\n \n*\n \nT\n),\n\n        \ntest_set_length\n:\n \nint\n(\n0.2\n \n*\n \nT\n)}\n\n\n\n\n\n\nTo generate a stimulus set:\n\n\nstim_set\n \n=\n \nStimulusSet\n()\n\n\nstim_set\n.\ngenerate_datasets\n(\nstim_pars\n)\n\n\n\n\n\n\nInput\n\n\nSpecifies the stimulus transduction and/or the input signal properties:\n\n\ninput_pars\n \n=\n \n{\n\n    \nsignal\n:\n \n{\n\n        \nN\n:\n \n3\n,\n     \n# Dimensionality of the signal \n\n        \ndurations\n:\n    \n# Duration of each stimulus presentation. \n\n            \n# Various settings are allowed:\n\n            \n# - List of n_trials elements whose values correspond to the \n\n            \n# duration of 1 stimulus presentation\n\n            \n# - List with one single element (uniform durations)\n\n            \n# - Tuple of (function, function parameters) specifying \n\n            \n# distributions for these values, \n\n            \n# - List of N lists of any of the formats specified... (*)\n\n        \n[(\nnp\n.\nrandom\n.\nuniform\n,\n \n{\nlow\n:\n \n500.\n,\n \nhigh\n:\n \n500.\n,\n \nsize\n:\n \nn_trials\n})],\n\n\n        \ni_stim_i\n:\n \n# Inter-stimulus intervals - same settings as \ndurations\n.\n\n            \n[(\nnp\n.\nrandom\n.\nuniform\n,\n \n{\nlow\n:\n \n0.\n,\n \nhigh\n:\n \n0.\n,\n \nsize\n:\n \nn_trials\n-\n1\n})],\n\n\n        \nkernel\n:\n \n(\nbox\n,\n \n{}),\n  \n# input mask - implemented options:\n\n                \n# box({}), exp({\ntau\n}), double_exp({\ntau_1\n,\ntau_2\n}), \n\n                \n# gauss({\nmu\n, \nsigma\n})\n\n\n        \nstart_time\n:\n \n0.\n,\n           \n# global signal onset time\n\n        \nstop_time\n:\n \nsys\n.\nfloat_info\n.\nmax\n,\n    \n# global signal offset time\n\n\n        \nmax_amplitude\n:\n    \n# maximum signal amplitude - as in durations and i_stim_i,\n\n                    \n# can be a list of values, or a function with parameters\n\n            \n[(\nnp\n.\nrandom\n.\nuniform\n,\n \n{\nlow\n:\n \n10.\n,\n \nhigh\n:\n \n100.\n,\n \nsize\n:\n \nn_trials\n})],\n\n\n        \nmin_amplitude\n:\n \n0.\n,\n    \n# minimum amplitude - will be added to the signal\n\n        \nresolution\n:\n \n1.\n    \n# temporal resolution\n\n        \n},\n\n\n    \nnoise\n:\n \n{\n\n        \nN\n:\n \n3\n,\n \n# Dimensionality of the noise component (common or \n\n            \n# multiple independent noise sources)\n\n        \nnoise_source\n:\n \n[\nGWN\n],\n    \n# [list] - Type of noise:\n\n            \n#  - Either a string for \nOU\n, \nGWN\n\n            \n#  - or a function, e.g. np.random.uniform\n\n        \nnoise_pars\n:\n   \n# [dict] Parameters (specific for each type of noise):\n\n                    \n# - \nOU\n -\n {\ntau\n, \nsigma\n, \ny0\n}\n\n                    \n# - \nGWN\n -\n {\namplitude\n, \nmean\n, \nstd\n}\n\n                    \n# - function parameters dictionary (see function documentation)\n\n            \n{\namplitude\n:\n \n1.\n,\n \nmean\n:\n \n1.\n,\n \nstd\n:\n \n0.1\n},\n\n        \nrectify\n:\n \nTrue\n,\n    \n# [bool] - rectify noise component \n\n        \nstart_time\n:\n \n0.\n,\n   \n# global onset time (single value), \n\n            \n# or local onset times if multiple instances are required (list)\n\n        \nstop_time\n:\n \nsys\n.\nfloat_info\n.\nmax\n,\n    \n# global offset time (single value), or local offset#\n\n            \n# times, if multiple instances are required\n\n        \nresolution\n:\n \n1.\n,\n   \n# signal resolution (dt)}}\n\n\n\n\n\n\nNote: the chosen specifications of \u2019durations\u2019, \u2019i_stim_i\u2019 and \u2019max_amplitude\u2019\nmust be consistent, i.e., if \u2019durations\u2019 is provided as a single element list, the same format must\nbe applied to \u2019i_stim_i\u2019 and \u2019max_amplitude\u2019. (test)\n\n\ninputs\n \n=\n \nInputSignalSet\n(\nparameter_set\n,\n \nstim_set\n,\n \nonline\n=\nonline\n)\n\n\ninputs\n.\ngenerate_datasets\n(\nstim_set\n)\n\n\n\n\n\n\nEncoding\n\n\n-\n\n\nenc_layer\n \n=\n \nEncodingLayer\n(\nparameter_set\n.\nencoding_pars\n,\n \nsignal\n=\ninputs\n.\nfull_set_signal\n,\n \nonline\n=\non\n\n\nenc_layer\n.\nconnect\n(\nparameter_set\n.\nencoding_pars\n,\n \nnet\n)\n\n\n\n\n\n\nDecoding\n\n\n-\n\n\nnet\n.\nconnect_decoders\n(\nparameter_set\n.\ndecoding_pars\n)\n\n\nenc_layer\n.\nconnect_decoders\n(\nparameter_set\n.\nencoding_pars\n.\ninput_decoder\n)\n\n\n\n\n\n\nAnalysis\n\n\n-\n\n\n\n\n\n\n\n\nParameter presets\n\n\nFor convenience, to reduce the length and complexity of the parameter files and the likelihood of\naccidentally changing the values of fixed, commonly used parameter sets, we typically include a set of\npreset dictionaries and simple functions to retrieve them and to simplify the construction of parameters\nfiles (see examples and upcoming project files).", 
            "title": "Parameters file and parameters module"
        }, 
        {
            "location": "/parameters/#parameter-range-declarations-and-build-function", 
            "text": "(...)  return   dict ([( kernel_pars ,   kernel_pars ), \n              ( neuron_pars ,   neuron_pars ), \n              ( net_pars ,   net_pars ), \n              ( encoding_pars ,   encoding_pars )])", 
            "title": "Parameter range declarations and build function"
        }, 
        {
            "location": "/parameters/#parameter-types", 
            "text": "The output of the build_parameters function is a dictionary of ParameterSets, containing all the\nnecessary types of parameters to be used by the main experiment. Acceptable types (with examples\nof use) are:", 
            "title": "Parameter types"
        }, 
        {
            "location": "/parameters/#kernel", 
            "text": "Specify all relevant system and simulation parameters.  kernel_pars   =   ParameterSet ({ \n     resolution :   0.1 ,        # simulation resolution \n     sim_time :   1000. ,        # total simulation time (often not required) \n     transient_t :   0. ,        # transient time  \n     data_prefix :   data_label , \n     data_path :   paths [ system_name ][ data_path ], \n     mpl_path :   paths [ system_name ][ matplotlib_rc ], \n     overwrite_files :   True , \n     print_time :   ( system_name   ==   local ), \n     rng_seeds :   range ( msd   +   N_vp   +   1 ,   msd   +   2   *   N_vp   +   1 ), \n     grng_seed :   msd   +   N_vp , \n     total_num_virtual_procs :   N_vp , \n     local_num_threads :   16 , \n     np_seed :   np_seed , \n\n     system :   { \n         local :   ( system_name   ==   local ), \n         system_label :   system_name , \n         queueing_system :   paths [ system_name ][ queueing_system ], \n         jdf_template :   paths [ system_name ][ jdf_template ], \n         remote_directory :   paths [ system_name ][ remote_directory ], \n         jdf_fields :   { {{ script_folder }} :   , \n                        {{ nodes }} :   str ( system_pars [ nodes ]), \n                        {{ ppn }} :   str ( system_pars [ ppn ]), \n                        {{ mem }} :   str ( system_pars [ mem ]), \n                        {{ walltime }} :   system_pars [ walltime ], \n                        {{ queue }} :   system_pars [ queue ], \n                        {{ computation_script }} :   }}})   If correctly specified, these parameters can be used during the initial setup of the main simulations:  set_global_rcParams ( parameter_set . kernel_pars [ mpl_path ])  np . random . seed ( parameter_set . kernel_pars [ np_seed ])  nest . SetKernelStatus ( extract_nestvalid_dict ( kernel_pars . as_dict (),   param_type = kernel ))", 
            "title": "Kernel"
        }, 
        {
            "location": "/parameters/#neuron", 
            "text": "neuron model parameters, must stricly abide by the naming conventions of the NEST\nmodel requested. For example:  neuron_pars   =   { \n     neuron_name :   { \n         model :   iaf_psc_exp , \n         C_m :   250.0 , \n         E_L :   0.0 , \n         V_reset :   0.0 , \n         V_th :   15. , \n         t_ref :   2. , \n         tau_syn_ex :   2. , \n         tau_syn_in :   2. , \n         tau_m :   20. }}   Typically the neuron parameters are only used within the parameters file, as they will be placed\nin the network parameter dictionary, where the code will use them.", 
            "title": "Neuron"
        }, 
        {
            "location": "/parameters/#network", 
            "text": "Network parameters, specifying the composition, topology and which variables to record\nfrom the multiple populations:  net_pars   =   { \n     n_populations :   2 ,           # total number of populations \n     pop_names :   [ E ,   I ], \n     n_neurons :   [ 8000 ,   2000 ], \n     neuron_pars :   [ neuron_pars [ E ],   neuron_pars [ I ]], \n     randomize_neuron_pars :   [{ V_m :   ( np . random . uniform ,   { low :   0.0 ,   high :   15. })},   # if necessary \n                               { V_m :   ( np . random . uniform ,   { low :   0.0 ,   high :   15. })}], \n     topology :   [ False ,   False ], \n     topology_dict :   [ None ,   None ], \n     record_spikes :   [ True ,   True ], \n     spike_device_pars :   [ copy_dict ( rec_devices , \n                                     { model :   spike_detector , \n                                      record_to :   [ memory ], \n                                      label :   }), \n                           copy_dict ( rec_devices , \n                                     { model :   spike_detector , \n                                      record_to :   [ memory ], \n                                      label :   })], \n     record_analogs :   [ False ,   False ], \n     analog_device_pars :   [ None ,   None ]}   Note that the dimensionality of all the list parameters has to be equal to n_populations. If\nthese parameters are all correctly set, generating a network is as simple as:  net   =   Network ( net_pars )  net . connect_devices ()", 
            "title": "Network"
        }, 
        {
            "location": "/parameters/#connection", 
            "text": "Connectivity parameters, specifying the composition, topology and connectivity of\nthe multiple populations:  connection_pars   =   { \n     n_synapse_types :   4 ,     # [int] - total number of connections to establish  \n     synapse_types :      # [list of tuples] - each tuple corresponds to 1 connection and  \n                 # has the form (tget_pop_label, src_pop_label)    \n         [( E ,   E ),   ( E ,   I ),   ( I ,   E ),   ( I ,   I )],  \n     topology_dependent :     # [list of bool] - whether synaptic connections are  \n                 # established relying on nest.topology module \n         [ False ,   False ,   False ,   False ],  \n     models :         # [list of str] - names of the synapse models to realize on each \n                 # synapse type \n         [ static_synapse ,   static_synapse ,   static_synapse ,   static_synapse ],            \n     model_pars :         # [list of dict] - each entry in the list contains the synapse \n                 # dictionary (if necessary) for the corresponding synapse model \n         [ synapse_pars_dict ,   synapse_pars_dict ,   synapse_pars_dict ,   synapse_pars_dict ],      \n     weight_dist :    # [list of dict] -   \n         [ common_syn_specs [ weight ],   common_syn_specs [ weight ],  \n          common_syn_specs [ weight ],   common_syn_specs [ weight ]], \n     pre_computedW :   # Provide an externally specified connectivity matrix \n             # if pre-computed weights matrices are provided, delays also \n             # need to be pre-specified, as an array with the same dimensions \n             # as W, with delay values on each corresponding entry. \n             # Alternatively, if all delays are the same, just provide a \n             # single number. \n         [ None ,   None ,   None ,   None ], \n\n     delay_dist :   [ common_syn_specs [ delay ],   common_syn_specs [ delay ], \n                 common_syn_specs [ delay ],   common_syn_specs [ delay ]], \n\n     conn_specs :     # [list of dict] - for topological connections \n         [ conn_specs ,   conn_specs ,   conn_specs ,   conn_specs ], \n\n     syn_specs :   []   To connect the network, just provide this parameter set to the connect method of the Network\nobject (for complex connectivity schemes, it is worth setting the progress=True to accompany the\nconnection progress):  net . connect_populations ( parameter_set . connection_pars ,   progress = True )", 
            "title": "Connection"
        }, 
        {
            "location": "/parameters/#stimulus", 
            "text": "Stimulus or task parameters:  stim_pars   =   { \n         n_stim :   10 ,         # number of stimuli \n         elements :   [ A ,   B ,   C ,   ... ], \n         grammar :   None ,   #!! \n         full_set_length :   int ( T   +   T_discard ),   # total samples \n         transient_set_length :   int ( T_discard ),  \n         train_set_length :   int ( 0.8   *   T ), \n         test_set_length :   int ( 0.2   *   T )}   To generate a stimulus set:  stim_set   =   StimulusSet ()  stim_set . generate_datasets ( stim_pars )", 
            "title": "Stimulus"
        }, 
        {
            "location": "/parameters/#input", 
            "text": "Specifies the stimulus transduction and/or the input signal properties:  input_pars   =   { \n     signal :   { \n         N :   3 ,       # Dimensionality of the signal  \n         durations :      # Duration of each stimulus presentation.  \n             # Various settings are allowed: \n             # - List of n_trials elements whose values correspond to the  \n             # duration of 1 stimulus presentation \n             # - List with one single element (uniform durations) \n             # - Tuple of (function, function parameters) specifying  \n             # distributions for these values,  \n             # - List of N lists of any of the formats specified... (*) \n         [( np . random . uniform ,   { low :   500. ,   high :   500. ,   size :   n_trials })], \n\n         i_stim_i :   # Inter-stimulus intervals - same settings as  durations . \n             [( np . random . uniform ,   { low :   0. ,   high :   0. ,   size :   n_trials - 1 })], \n\n         kernel :   ( box ,   {}),    # input mask - implemented options: \n                 # box({}), exp({ tau }), double_exp({ tau_1 , tau_2 }),  \n                 # gauss({ mu ,  sigma }) \n\n         start_time :   0. ,             # global signal onset time \n         stop_time :   sys . float_info . max ,      # global signal offset time \n\n         max_amplitude :      # maximum signal amplitude - as in durations and i_stim_i, \n                     # can be a list of values, or a function with parameters \n             [( np . random . uniform ,   { low :   10. ,   high :   100. ,   size :   n_trials })], \n\n         min_amplitude :   0. ,      # minimum amplitude - will be added to the signal \n         resolution :   1.      # temporal resolution \n         }, \n\n     noise :   { \n         N :   3 ,   # Dimensionality of the noise component (common or  \n             # multiple independent noise sources) \n         noise_source :   [ GWN ],      # [list] - Type of noise: \n             #  - Either a string for  OU ,  GWN \n             #  - or a function, e.g. np.random.uniform \n         noise_pars :     # [dict] Parameters (specific for each type of noise): \n                     # -  OU  -  { tau ,  sigma ,  y0 } \n                     # -  GWN  -  { amplitude ,  mean ,  std } \n                     # - function parameters dictionary (see function documentation) \n             { amplitude :   1. ,   mean :   1. ,   std :   0.1 }, \n         rectify :   True ,      # [bool] - rectify noise component  \n         start_time :   0. ,     # global onset time (single value),  \n             # or local onset times if multiple instances are required (list) \n         stop_time :   sys . float_info . max ,      # global offset time (single value), or local offset# \n             # times, if multiple instances are required \n         resolution :   1. ,     # signal resolution (dt)}}   Note: the chosen specifications of \u2019durations\u2019, \u2019i_stim_i\u2019 and \u2019max_amplitude\u2019\nmust be consistent, i.e., if \u2019durations\u2019 is provided as a single element list, the same format must\nbe applied to \u2019i_stim_i\u2019 and \u2019max_amplitude\u2019. (test)  inputs   =   InputSignalSet ( parameter_set ,   stim_set ,   online = online )  inputs . generate_datasets ( stim_set )", 
            "title": "Input"
        }, 
        {
            "location": "/parameters/#encoding", 
            "text": "-  enc_layer   =   EncodingLayer ( parameter_set . encoding_pars ,   signal = inputs . full_set_signal ,   online = on  enc_layer . connect ( parameter_set . encoding_pars ,   net )", 
            "title": "Encoding"
        }, 
        {
            "location": "/parameters/#decoding", 
            "text": "-  net . connect_decoders ( parameter_set . decoding_pars )  enc_layer . connect_decoders ( parameter_set . encoding_pars . input_decoder )", 
            "title": "Decoding"
        }, 
        {
            "location": "/parameters/#analysis", 
            "text": "-", 
            "title": "Analysis"
        }, 
        {
            "location": "/parameters/#parameter-presets", 
            "text": "For convenience, to reduce the length and complexity of the parameter files and the likelihood of\naccidentally changing the values of fixed, commonly used parameter sets, we typically include a set of\npreset dictionaries and simple functions to retrieve them and to simplify the construction of parameters\nfiles (see examples and upcoming project files).", 
            "title": "Parameter presets"
        }, 
        {
            "location": "/input/", 
            "text": "The input_architect module handles everything related to input stimuli and signals. It is\ndesigned to encompass a large variety of input stimuli / signals, patterned according to complex\nspecifications. However, at the moment, due to recent changes, not all variants have been tested.\nNone of the following components of the input construction process is strictly necessary. For example,\na signal can be generated using the signal_pars, without the need for a StimulusSet to be specified.\nThe main classes are:\n\n\n\n\nStimulusSet\n \u2013 hold and manipulate all the data pertaining to the input stimuli, labels, and\ncorresponding time series, can be divided into data sets (transient, train and test sets)\n\n\nInputSignalSet\n \u2013 container for all the relevant signals, divided into data sets (transient, train\nand test sets)\n\n\nInputSignal\n \u2013 Generate and store AnalogSignal object referring to the structured input\nsignal u(t)\n\n\nInputNoise\n \u2013 Generate and store AnalogSignal object referring to the noise signal\n(...)", 
            "title": "Input specification and generation"
        }, 
        {
            "location": "/encoding/", 
            "text": "The EncodingLayer class wraps all the encoding process. It\u2019s main constituents are:\n\n\n\n\nGenerator\n \u2013 consists of a NEST generator device (e.g. spike_generator, step_current_generator,\ninh_poisson_generator). Note that in the parameter specifications, N refers to the num-\nber of unique devices in the setup not the number of devices of a certain type (this is later\nacquired by the dimensionality of the input signal to be encoded - 1 unique generator device\nper input dimension)\n\n\nEncoder\n \u2013 consists of a Population object containing a layer of spiking neurons or some other\nmechanism that converts the continuous input into spike trains to be fed to the network", 
            "title": "Encoding layer"
        }, 
        {
            "location": "/populations-and-networks/", 
            "text": "(...)", 
            "title": "Populations and networks"
        }, 
        {
            "location": "/decoding/", 
            "text": "Specifying state variables\n\n\nIn the current implementation, one can simultaneously record and readout all relevant variables, for\ncomparison purposes (as long as they are recordable). To do so, we just specify which populations\nto read, and which variable to read from. This is done in the dictionary that is passed as argu-\nment to the decoding defaults (see below), namely the key variables \ndecoded_population\n and\n\nstate_variable\n.\n\n\nThese two variables need to be lists of equal length. A given decoded_population can be\nspecified as a sub-list, which means that we want to extract 1 state matrix from the combination of\nthe two referred populations (which are merged for this purpose). See example below..\n\n\nState sampling methods\n\n\nThe population responses to the stimuli can also be extracted in various different ways, by taking\nsamples of the state variable under consideration. In the current implementation this is simply done\nwith the decoder variable global_sampling_times:\n\n\n\n\nOne-sample at the end of each stimulus (default, t* ):\n\n\nglobal_sampling_times = None\n\n\none population state vector per stimulus\n\n\n\n\n\n\nMultiple samples taken at specific times (all stimuli must have the same duration, or at least\nthe sample times cannot fall outside the stimulus duration):\n\n\nglobal_sampling_times is a list of np.array of times (from stimulus onset) - length\nN*\n\n\none population state vector per sampling time\n\n\nconstructs N * state matrices that will be independently readout\n\n\n\n\n\n\nSub-sampling responses at a fixed rate:\n\n\nglobal_sampling_times = 1/10 is a fraction (one sample every 10 steps, step\nsize being the input resolution)\n\n\nconstructs one long state matrix corresponding to the full response (in the limit, if\nglobal_sampling_times = 1/10, this corresponds to the entire activity history)\n\n\nthe target is also downsampled at the same rate, implementing a kind of continuous\nreadout..\n\n\n\n\n\n\n\n\nThese different methods are used for different purposes, to assess different features of the responses. See Examples.\n\n\nReadouts\n\n\nCurrently available algorithms are:\n\n\n\n\nDirect pseudo-inverse - \n'pinv'\n\n\nRidge Regression - \n'ridge'\n\n\nLogistic Regression - \n'logistic'\n\n\nPerceptron - \n'perceptron'\n\n\nLinear SVM - \n'svm-linear'\n\n\nNon-linear SVM (radial basis function) - \n'svm-rbf'\n\n\n\n\ndecoders\n \n=\n \ndict\n(\n\n    \ndecoded_population\n=\n[\nE\n,\n \nE\n,\n \n[\nE\n,\n \nI\n],\n \n[\nE\n,\n \nI\n],\n \nI\n,\n \nI\n],\n\n    \nstate_variable\n=\n[\nV_m\n,\n \nspikes\n,\n \nV_m\n,\n \nspikes\n,\n \nV_m\n,\n \nspikes\n],\n\n    \nfilter_time\n=\nfilter_tau\n,\n\n    \nreadouts\n=\nreadout_labels\n,\n\n    \nreadout_algorithms\n=\n[\nridge\n,\n \nridge\n,\n \nridge\n,\n \nridge\n,\n \nridge\n,\n \nridge\n],\n\n    \nglobal_sampling_times\n=\nstate_sampling\n,\n\n\n)\n\n\ndecoding_pars\n \n=\n \nset_decoding_defaults\n(\ndefault_set\n=\n1\n,\n \n                                      \noutput_resolution\n=\n1.\n,\n \n                                      \nto_memory\n=\nTrue\n,\n \n                                      \nkernel_pars\n=\nkernel_pars\n,\n\n                                      \n**\ndecoders\n)", 
            "title": "Decoding Layer"
        }, 
        {
            "location": "/decoding/#specifying-state-variables", 
            "text": "In the current implementation, one can simultaneously record and readout all relevant variables, for\ncomparison purposes (as long as they are recordable). To do so, we just specify which populations\nto read, and which variable to read from. This is done in the dictionary that is passed as argu-\nment to the decoding defaults (see below), namely the key variables  decoded_population  and state_variable .  These two variables need to be lists of equal length. A given decoded_population can be\nspecified as a sub-list, which means that we want to extract 1 state matrix from the combination of\nthe two referred populations (which are merged for this purpose). See example below..", 
            "title": "Specifying state variables"
        }, 
        {
            "location": "/decoding/#state-sampling-methods", 
            "text": "The population responses to the stimuli can also be extracted in various different ways, by taking\nsamples of the state variable under consideration. In the current implementation this is simply done\nwith the decoder variable global_sampling_times:   One-sample at the end of each stimulus (default, t* ):  global_sampling_times = None  one population state vector per stimulus    Multiple samples taken at specific times (all stimuli must have the same duration, or at least\nthe sample times cannot fall outside the stimulus duration):  global_sampling_times is a list of np.array of times (from stimulus onset) - length\nN*  one population state vector per sampling time  constructs N * state matrices that will be independently readout    Sub-sampling responses at a fixed rate:  global_sampling_times = 1/10 is a fraction (one sample every 10 steps, step\nsize being the input resolution)  constructs one long state matrix corresponding to the full response (in the limit, if\nglobal_sampling_times = 1/10, this corresponds to the entire activity history)  the target is also downsampled at the same rate, implementing a kind of continuous\nreadout..     These different methods are used for different purposes, to assess different features of the responses. See Examples.", 
            "title": "State sampling methods"
        }, 
        {
            "location": "/decoding/#readouts", 
            "text": "Currently available algorithms are:   Direct pseudo-inverse -  'pinv'  Ridge Regression -  'ridge'  Logistic Regression -  'logistic'  Perceptron -  'perceptron'  Linear SVM -  'svm-linear'  Non-linear SVM (radial basis function) -  'svm-rbf'   decoders   =   dict ( \n     decoded_population = [ E ,   E ,   [ E ,   I ],   [ E ,   I ],   I ,   I ], \n     state_variable = [ V_m ,   spikes ,   V_m ,   spikes ,   V_m ,   spikes ], \n     filter_time = filter_tau , \n     readouts = readout_labels , \n     readout_algorithms = [ ridge ,   ridge ,   ridge ,   ridge ,   ridge ,   ridge ], \n     global_sampling_times = state_sampling ,  )  decoding_pars   =   set_decoding_defaults ( default_set = 1 ,  \n                                       output_resolution = 1. ,  \n                                       to_memory = True ,  \n                                       kernel_pars = kernel_pars , \n                                       ** decoders )", 
            "title": "Readouts"
        }, 
        {
            "location": "/analysis-and-visualization/", 
            "text": "", 
            "title": "Analysis and visualization"
        }, 
        {
            "location": "/release-notes/", 
            "text": "Release notes\n\n\nUpgrading\n\n\nTo upgrade Material to the latest version, use pip:\n\n\npip install --upgrade mkdocs-material\n\n\n\n\n\nTo determine the currently installed version, use the following command:\n\n\npip show mkdocs-material \n|\n grep -E ^Version\n\n# Version 1.6.1\n\n\n\n\n\n\nChangelog\n\n\n1.6.1 \n _ April 23, 2017\n\n\n\n\nFixed following of active/focused element if search input is focused\n\n\nFixed layer order of search component elements\n\n\n\n\n1.6.0 \n _ April 22, 2017\n\n\n\n\nAdded build test for Docker image on Travis\n\n\nAdded search overlay for better user experience (focus)\n\n\nAdded language from localizations to \nhtml\n tag\n\n\nFixed \n#270\n: source links broken for absolute URLs\n\n\nFixed missing top spacing for first targeted element in content\n\n\nFixed too small footnote divider when using larger font sizes\n\n\n\n\n1.5.5 \n _ April 20, 2017\n\n\n\n\nFixed \n#282\n: Browser search (\nMeta\n+\nF\n) is hijacked\n\n\n\n\n1.5.4 \n _ April 8, 2017\n\n\n\n\nFixed broken highlighting for two or more search terms\n\n\nFixed missing search results when only a h1 is present\n\n\nFixed unresponsive overlay on Android\n\n\n\n\n1.5.3 \n _ April 7, 2017\n\n\n\n\nFixed deprecated calls for template variables\n\n\nFixed wrong palette color for focused search result\n\n\nFixed JavaScript errors on 404 page\n\n\nFixed missing top spacing on 404 page\n\n\nFixed missing right spacing on overflow of source container\n\n\n\n\n1.5.2 \n _ April 5, 2017\n\n\n\n\nAdded requirements as explicit dependencies in \nsetup.py\n\n\nFixed non-synchronized transitions in search form\n\n\n\n\n1.5.1 \n _ March 30, 2017\n\n\n\n\nFixed rendering and offset of targetted footnotes\n\n\nFixed \n#238\n: Link on logo is not set to \nsite_url\n\n\n\n\n1.5.0 \n _ March 24, 2017\n\n\n\n\nAdded support for localization of search placeholder\n\n\nAdded keyboard events for quick access of search\n\n\nAdded keyboard events for search control\n\n\nAdded opacity on hover for search buttons\n\n\nAdded git hook to skip CI build on non-src changes\n\n\nFixed non-resetting search placeholder when input is cleared\n\n\nFixed error for unescaped parentheses in search term\n\n\nFixed \n#229\n: Button to clear search missing\n\n\nFixed \n#231\n: Escape key doesn't exit search\n\n\nRemoved old-style figures from font feature settings\n\n\n\n\n1.4.1 \n _ March 16, 2017\n\n\n\n\nFixed invalid destructuring attempt on NodeList (in Safari, Edge, IE)\n\n\n\n\n1.4.0 \n _ March 16, 2017\n\n\n\n\nAdded support for grouping searched sections by documents\n\n\nAdded support for highlighting of search terms\n\n\nAdded support for localization of search results\n\n\nFixed \n#216\n: table of contents icon doesn't show if \nh1\n is not present\n\n\nReworked style and layout of search results for better usability\n\n\n\n\n1.3.0 \n _ March 11, 2017\n\n\n\n\nAdded support for page-specific title and description using metadata\n\n\nAdded support for linking source files to documentation\n\n\nFixed jitter and offset of sidebar when zooming browser\n\n\nFixed incorrectly initialized tablet sidebar height\n\n\nFixed regression for \n#1\n: GitHub stars break if \nrepo_url\n ends with a \n/\n\n\nFixed undesired white line below copyright footer due to base font scaling\n\n\nFixed issue with whitespace in path for scripts\n\n\nFixed \n#205\n: support non-fixed (static) header\n\n\nRefactored footnote references for better visibility\n\n\nReduced repaints to a minimum for non-tabs configuration\n\n\nReduced contrast of edit button (slightly)\n\n\n\n\n1.2.0 \n _ March 3, 2017\n\n\n\n\nAdded \nquote\n (synonym: \ncite\n) style for Admonition\n\n\nAdded help message to build pipeline\n\n\nFixed wrong navigation link colors when applying palette\n\n\nFixed \n#197\n: Link missing in tabs navigation on deeply nested items\n\n\nRemoved unnecessary dev dependencies\n\n\n\n\n1.1.1 \n _ February 26, 2017\n\n\n\n\nFixed incorrectly displayed nested lists when using tabs\n\n\n\n\n1.1.0 \n _ February 26, 2017\n\n\n\n\nAdded tabs navigation feature (optional)\n\n\nAdded Disqus integration (optional)\n\n\nAdded a high resolution Favicon with the new logo\n\n\nAdded static type checking using Facebook's Flow\n\n\nFixed \n#173\n: Dictionary elements have no bottom spacing\n\n\nFixed \n#175\n: Tables cannot be set to 100% width\n\n\nFixed race conditions in build related to asset revisioning\n\n\nFixed accidentally re-introduced Permalink on top-level headline\n\n\nFixed alignment of logo in drawer on IE11\n\n\nRefactored styles related to tables\n\n\nRefactored and automated Docker build and PyPI release\n\n\nRefactored build scripts\n\n\n\n\n1.0.5 \n _ February 18, 2017\n\n\n\n\nFixed \n#153\n: Sidebar flows out of constrained area in Chrome 56\n\n\nFixed \n#159\n: Footer jitter due to JavaScript if content is short\n\n\n\n\n1.0.4 \n _ February 16, 2017\n\n\n\n\nFixed \n#142\n: Documentation build errors if \nh1\n is defined as raw HTML\n\n\nFixed \n#164\n: PyPI release does not build and install\n\n\nFixed offsets of targeted headlines\n\n\nIncreased sidebar font size by \n0.12rem\n\n\n\n\n1.0.3 \n _ January 22, 2017\n\n\n\n\nFixed \n#117\n: Table of contents items don't blur on fast scrolling\n\n\nRefactored sidebar positioning logic\n\n\nFurther reduction of repaints\n\n\n\n\n1.0.2 \n _ January 15, 2017\n\n\n\n\nFixed \n#108\n: Horizontal scrollbar in content area\n\n\n\n\n1.0.1 \n _ January 14, 2017\n\n\n\n\nFixed massive repaints happening when scrolling\n\n\nFixed footer back reference positions in case of overflow\n\n\nFixed header logo from showing when the menu icon is rendered\n\n\nChanged scrollbar behavior to only show when content overflows\n\n\n\n\n1.0.0 \n _ January 13, 2017\n\n\n\n\nIntroduced Webpack for more sophisticated JavaScript bundling\n\n\nIntroduced ESLint and Stylelint for code style checks\n\n\nIntroduced more accurate Material Design colors and shadows\n\n\nIntroduced modular scales for harmonic font sizing\n\n\nIntroduced git-hooks for better development workflow\n\n\nRewrite of CSS using the BEM methodology and SassDoc guidelines\n\n\nRewrite of JavaScript using ES6 and Babel as a transpiler\n\n\nRewrite of Admonition, Permalinks and CodeHilite integration\n\n\nRewrite of the complete typographical system\n\n\nRewrite of Gulp asset pipeline in ES6 and separation of tasks\n\n\nRemoved Bower as a dependency in favor of NPM\n\n\nRemoved custom icon build in favor of the Material Design iconset\n\n\nRemoved \n_blank\n targets on links due to vulnerability: \nhttp://bit.ly/1Mk2Rtw\n\n\nRemoved unversioned assets from build directory\n\n\nRestructured templates into base templates and partials\n\n\nAdded build and watch scripts in \npackage.json\n\n\nAdded support for Metadata and Footnotes Markdown extensions\n\n\nAdded support for PyMdown Extensions package\n\n\nAdded support for collapsible sections in navigation\n\n\nAdded support for separate table of contents\n\n\nAdded support for better accessibility through REM-based layout\n\n\nAdded icons for GitHub, GitLab and BitBucket integrations\n\n\nAdded more detailed documentation on specimen, extensions etc.\n\n\nAdded a \n404.html\n error page for deployment on GitHub Pages\n\n\nFixed live reload chain in watch mode when saving a template\n\n\nFixed variable references to work with MkDocs 0.16\n\n\n\n\n0.2.4 \n _ June 26, 2016\n\n\n\n\nFixed improperly set default favicon\n\n\nFixed \n#33\n: Protocol relative URL for webfonts doesn't work with\n  \nfile://\n\n\nFixed \n#34\n: IE11 on Windows 7 doesn't honor \nmax-width\n on \nmain\n tag\n\n\nFixed \n#35\n: Add styling for blockquotes\n\n\n\n\n0.2.3 \n _ May 16, 2016\n\n\n\n\nFixed \n#25\n: Highlight inline fenced blocks\n\n\nFixed \n#26\n: Better highlighting for keystrokes\n\n\nFixed \n#30\n: Suboptimal syntax highlighting for PHP\n\n\n\n\n0.2.2 \n _ March 20, 2016\n\n\n\n\nFixed \n#15\n: Document Pygments dependency for CodeHilite\n\n\nFixed \n#16\n: Favicon could not be set through \nmkdocs.yml\n\n\nFixed \n#17\n: Put version into own container for styling\n\n\nFixed \n#20\n: Fix rounded borders for tables\n\n\n\n\n0.2.1 \n _ March 12, 2016\n\n\n\n\nFixed \n#10\n: Invisible header after closing search bar with\n  \nESC\n key\n\n\nFixed \n#13\n: Table cells don't wrap\n\n\nFixed empty list in table of contents when no headline is defined\n\n\nCorrected wrong path for static asset monitoring in Gulpfile.js\n\n\nSet up tracking of site search for Google Analytics\n\n\n\n\n0.2.0 \n _ February 24, 2016\n\n\n\n\nFixed \n#6\n: Include multiple color palettes via \nmkdocs.yml\n\n\nFixed \n#7\n: Better colors for links inside admonition notes and warnings\n\n\nFixed \n#9\n: Text for prev/next footer navigation should be customizable\n\n\nRefactored templates (replaced \nif\n/\nelse\n with modifiers where possible)\n\n\n\n\n0.1.3 \n _ February 21, 2016\n\n\n\n\nFixed \n#3\n: Ordered lists within an unordered list have \n::before\n content\n\n\nFixed \n#4\n: Click on Logo/Title without Github-Repository: \nNone\n\n\nFixed \n#5\n: Page without headlines renders empty list in table of contents\n\n\nMoved Modernizr to top to ensure basic usability in IE8\n\n\n\n\n0.1.2 \n _ February 16, 2016\n\n\n\n\nFixed styles for deep navigational hierarchies\n\n\nFixed webfont delivery problem when hosted in subdirectories\n\n\nFixed print styles in mobile/tablet configuration\n\n\nAdded option to configure fonts in \nmkdocs.yml\n with fallbacks\n\n\nChanged styles for admonition notes and warnings\n\n\nSet download link to latest version if available\n\n\nSet up tracking of outgoing links and actions for Google Analytics\n\n\n\n\n0.1.1 \n _ February 11, 2016\n\n\n\n\nFixed \n#1\n: GitHub stars don't work if the repo_url ends with a \n/\n\n\nUpdated NPM and Bower dependencies to most recent versions\n\n\nChanged footer/copyright link to Material theme to GitHub pages\n\n\nMade MkDocs building/serving in build process optional\n\n\nSet up continuous integration with \nTravis\n\n\n\n\n0.1.0 \n _ February 9, 2016\n\n\n\n\nInitial release", 
            "title": "Release notes"
        }, 
        {
            "location": "/release-notes/#release-notes", 
            "text": "", 
            "title": "Release notes"
        }, 
        {
            "location": "/release-notes/#upgrading", 
            "text": "To upgrade Material to the latest version, use pip:  pip install --upgrade mkdocs-material  To determine the currently installed version, use the following command:  pip show mkdocs-material  |  grep -E ^Version # Version 1.6.1", 
            "title": "Upgrading"
        }, 
        {
            "location": "/release-notes/#changelog", 
            "text": "", 
            "title": "Changelog"
        }, 
        {
            "location": "/release-notes/#161-_-april-23-2017", 
            "text": "Fixed following of active/focused element if search input is focused  Fixed layer order of search component elements", 
            "title": "1.6.1  _ April 23, 2017"
        }, 
        {
            "location": "/release-notes/#160-_-april-22-2017", 
            "text": "Added build test for Docker image on Travis  Added search overlay for better user experience (focus)  Added language from localizations to  html  tag  Fixed  #270 : source links broken for absolute URLs  Fixed missing top spacing for first targeted element in content  Fixed too small footnote divider when using larger font sizes", 
            "title": "1.6.0  _ April 22, 2017"
        }, 
        {
            "location": "/release-notes/#155-_-april-20-2017", 
            "text": "Fixed  #282 : Browser search ( Meta + F ) is hijacked", 
            "title": "1.5.5  _ April 20, 2017"
        }, 
        {
            "location": "/release-notes/#154-_-april-8-2017", 
            "text": "Fixed broken highlighting for two or more search terms  Fixed missing search results when only a h1 is present  Fixed unresponsive overlay on Android", 
            "title": "1.5.4  _ April 8, 2017"
        }, 
        {
            "location": "/release-notes/#153-_-april-7-2017", 
            "text": "Fixed deprecated calls for template variables  Fixed wrong palette color for focused search result  Fixed JavaScript errors on 404 page  Fixed missing top spacing on 404 page  Fixed missing right spacing on overflow of source container", 
            "title": "1.5.3  _ April 7, 2017"
        }, 
        {
            "location": "/release-notes/#152-_-april-5-2017", 
            "text": "Added requirements as explicit dependencies in  setup.py  Fixed non-synchronized transitions in search form", 
            "title": "1.5.2  _ April 5, 2017"
        }, 
        {
            "location": "/release-notes/#151-_-march-30-2017", 
            "text": "Fixed rendering and offset of targetted footnotes  Fixed  #238 : Link on logo is not set to  site_url", 
            "title": "1.5.1  _ March 30, 2017"
        }, 
        {
            "location": "/release-notes/#150-_-march-24-2017", 
            "text": "Added support for localization of search placeholder  Added keyboard events for quick access of search  Added keyboard events for search control  Added opacity on hover for search buttons  Added git hook to skip CI build on non-src changes  Fixed non-resetting search placeholder when input is cleared  Fixed error for unescaped parentheses in search term  Fixed  #229 : Button to clear search missing  Fixed  #231 : Escape key doesn't exit search  Removed old-style figures from font feature settings", 
            "title": "1.5.0  _ March 24, 2017"
        }, 
        {
            "location": "/release-notes/#141-_-march-16-2017", 
            "text": "Fixed invalid destructuring attempt on NodeList (in Safari, Edge, IE)", 
            "title": "1.4.1  _ March 16, 2017"
        }, 
        {
            "location": "/release-notes/#140-_-march-16-2017", 
            "text": "Added support for grouping searched sections by documents  Added support for highlighting of search terms  Added support for localization of search results  Fixed  #216 : table of contents icon doesn't show if  h1  is not present  Reworked style and layout of search results for better usability", 
            "title": "1.4.0  _ March 16, 2017"
        }, 
        {
            "location": "/release-notes/#130-_-march-11-2017", 
            "text": "Added support for page-specific title and description using metadata  Added support for linking source files to documentation  Fixed jitter and offset of sidebar when zooming browser  Fixed incorrectly initialized tablet sidebar height  Fixed regression for  #1 : GitHub stars break if  repo_url  ends with a  /  Fixed undesired white line below copyright footer due to base font scaling  Fixed issue with whitespace in path for scripts  Fixed  #205 : support non-fixed (static) header  Refactored footnote references for better visibility  Reduced repaints to a minimum for non-tabs configuration  Reduced contrast of edit button (slightly)", 
            "title": "1.3.0  _ March 11, 2017"
        }, 
        {
            "location": "/release-notes/#120-_-march-3-2017", 
            "text": "Added  quote  (synonym:  cite ) style for Admonition  Added help message to build pipeline  Fixed wrong navigation link colors when applying palette  Fixed  #197 : Link missing in tabs navigation on deeply nested items  Removed unnecessary dev dependencies", 
            "title": "1.2.0  _ March 3, 2017"
        }, 
        {
            "location": "/release-notes/#111-_-february-26-2017", 
            "text": "Fixed incorrectly displayed nested lists when using tabs", 
            "title": "1.1.1  _ February 26, 2017"
        }, 
        {
            "location": "/release-notes/#110-_-february-26-2017", 
            "text": "Added tabs navigation feature (optional)  Added Disqus integration (optional)  Added a high resolution Favicon with the new logo  Added static type checking using Facebook's Flow  Fixed  #173 : Dictionary elements have no bottom spacing  Fixed  #175 : Tables cannot be set to 100% width  Fixed race conditions in build related to asset revisioning  Fixed accidentally re-introduced Permalink on top-level headline  Fixed alignment of logo in drawer on IE11  Refactored styles related to tables  Refactored and automated Docker build and PyPI release  Refactored build scripts", 
            "title": "1.1.0  _ February 26, 2017"
        }, 
        {
            "location": "/release-notes/#105-_-february-18-2017", 
            "text": "Fixed  #153 : Sidebar flows out of constrained area in Chrome 56  Fixed  #159 : Footer jitter due to JavaScript if content is short", 
            "title": "1.0.5  _ February 18, 2017"
        }, 
        {
            "location": "/release-notes/#104-_-february-16-2017", 
            "text": "Fixed  #142 : Documentation build errors if  h1  is defined as raw HTML  Fixed  #164 : PyPI release does not build and install  Fixed offsets of targeted headlines  Increased sidebar font size by  0.12rem", 
            "title": "1.0.4  _ February 16, 2017"
        }, 
        {
            "location": "/release-notes/#103-_-january-22-2017", 
            "text": "Fixed  #117 : Table of contents items don't blur on fast scrolling  Refactored sidebar positioning logic  Further reduction of repaints", 
            "title": "1.0.3  _ January 22, 2017"
        }, 
        {
            "location": "/release-notes/#102-_-january-15-2017", 
            "text": "Fixed  #108 : Horizontal scrollbar in content area", 
            "title": "1.0.2  _ January 15, 2017"
        }, 
        {
            "location": "/release-notes/#101-_-january-14-2017", 
            "text": "Fixed massive repaints happening when scrolling  Fixed footer back reference positions in case of overflow  Fixed header logo from showing when the menu icon is rendered  Changed scrollbar behavior to only show when content overflows", 
            "title": "1.0.1  _ January 14, 2017"
        }, 
        {
            "location": "/release-notes/#100-_-january-13-2017", 
            "text": "Introduced Webpack for more sophisticated JavaScript bundling  Introduced ESLint and Stylelint for code style checks  Introduced more accurate Material Design colors and shadows  Introduced modular scales for harmonic font sizing  Introduced git-hooks for better development workflow  Rewrite of CSS using the BEM methodology and SassDoc guidelines  Rewrite of JavaScript using ES6 and Babel as a transpiler  Rewrite of Admonition, Permalinks and CodeHilite integration  Rewrite of the complete typographical system  Rewrite of Gulp asset pipeline in ES6 and separation of tasks  Removed Bower as a dependency in favor of NPM  Removed custom icon build in favor of the Material Design iconset  Removed  _blank  targets on links due to vulnerability:  http://bit.ly/1Mk2Rtw  Removed unversioned assets from build directory  Restructured templates into base templates and partials  Added build and watch scripts in  package.json  Added support for Metadata and Footnotes Markdown extensions  Added support for PyMdown Extensions package  Added support for collapsible sections in navigation  Added support for separate table of contents  Added support for better accessibility through REM-based layout  Added icons for GitHub, GitLab and BitBucket integrations  Added more detailed documentation on specimen, extensions etc.  Added a  404.html  error page for deployment on GitHub Pages  Fixed live reload chain in watch mode when saving a template  Fixed variable references to work with MkDocs 0.16", 
            "title": "1.0.0  _ January 13, 2017"
        }, 
        {
            "location": "/release-notes/#024-_-june-26-2016", 
            "text": "Fixed improperly set default favicon  Fixed  #33 : Protocol relative URL for webfonts doesn't work with\n   file://  Fixed  #34 : IE11 on Windows 7 doesn't honor  max-width  on  main  tag  Fixed  #35 : Add styling for blockquotes", 
            "title": "0.2.4  _ June 26, 2016"
        }, 
        {
            "location": "/release-notes/#023-_-may-16-2016", 
            "text": "Fixed  #25 : Highlight inline fenced blocks  Fixed  #26 : Better highlighting for keystrokes  Fixed  #30 : Suboptimal syntax highlighting for PHP", 
            "title": "0.2.3  _ May 16, 2016"
        }, 
        {
            "location": "/release-notes/#022-_-march-20-2016", 
            "text": "Fixed  #15 : Document Pygments dependency for CodeHilite  Fixed  #16 : Favicon could not be set through  mkdocs.yml  Fixed  #17 : Put version into own container for styling  Fixed  #20 : Fix rounded borders for tables", 
            "title": "0.2.2  _ March 20, 2016"
        }, 
        {
            "location": "/release-notes/#021-_-march-12-2016", 
            "text": "Fixed  #10 : Invisible header after closing search bar with\n   ESC  key  Fixed  #13 : Table cells don't wrap  Fixed empty list in table of contents when no headline is defined  Corrected wrong path for static asset monitoring in Gulpfile.js  Set up tracking of site search for Google Analytics", 
            "title": "0.2.1  _ March 12, 2016"
        }, 
        {
            "location": "/release-notes/#020-_-february-24-2016", 
            "text": "Fixed  #6 : Include multiple color palettes via  mkdocs.yml  Fixed  #7 : Better colors for links inside admonition notes and warnings  Fixed  #9 : Text for prev/next footer navigation should be customizable  Refactored templates (replaced  if / else  with modifiers where possible)", 
            "title": "0.2.0  _ February 24, 2016"
        }, 
        {
            "location": "/release-notes/#013-_-february-21-2016", 
            "text": "Fixed  #3 : Ordered lists within an unordered list have  ::before  content  Fixed  #4 : Click on Logo/Title without Github-Repository:  None  Fixed  #5 : Page without headlines renders empty list in table of contents  Moved Modernizr to top to ensure basic usability in IE8", 
            "title": "0.1.3  _ February 21, 2016"
        }, 
        {
            "location": "/release-notes/#012-_-february-16-2016", 
            "text": "Fixed styles for deep navigational hierarchies  Fixed webfont delivery problem when hosted in subdirectories  Fixed print styles in mobile/tablet configuration  Added option to configure fonts in  mkdocs.yml  with fallbacks  Changed styles for admonition notes and warnings  Set download link to latest version if available  Set up tracking of outgoing links and actions for Google Analytics", 
            "title": "0.1.2  _ February 16, 2016"
        }, 
        {
            "location": "/release-notes/#011-_-february-11-2016", 
            "text": "Fixed  #1 : GitHub stars don't work if the repo_url ends with a  /  Updated NPM and Bower dependencies to most recent versions  Changed footer/copyright link to Material theme to GitHub pages  Made MkDocs building/serving in build process optional  Set up continuous integration with  Travis", 
            "title": "0.1.1  _ February 11, 2016"
        }, 
        {
            "location": "/release-notes/#010-_-february-9-2016", 
            "text": "Initial release", 
            "title": "0.1.0  _ February 9, 2016"
        }, 
        {
            "location": "/contributing/", 
            "text": "Contributing\n\n\nInterested in contributing to the Material theme? Want to report a bug? Before\nyou do, please read the following guidelines.\n\n\nSubmission context\n\n\nGot a question or problem?\n\n\nFor quick questions there's no need to open an issue as you can reach us on\n\ngitter.im\n.\n\n\nFound a bug?\n\n\nIf you found a bug in the source code, you can help us by submitting an issue\nto the \nissue tracker\n in our GitHub repository. Even better, you can submit\na Pull Request with a fix. However, before doing so, please read the\n\nsubmission guidelines\n.\n\n\nMissing a feature?\n\n\nYou can request a new feature by submitting an issue to our GitHub Repository.\nIf you would like to implement a new feature, please submit an issue with a\nproposal for your work first, to be sure that it is of use for everyone, as\nthe Material theme is highly opinionated. Please consider what kind of change\nit is:\n\n\n\n\n\n\nFor a \nmajor feature\n, first open an issue and outline your proposal so\n  that it can be discussed. This will also allow us to better coordinate our\n  efforts, prevent duplication of work, and help you to craft the change so\n  that it is successfully accepted into the project.\n\n\n\n\n\n\nSmall features and bugs\n can be crafted and directly submitted as a Pull\n  Request. However, there is no guarantee that your feature will make it into\n  the master, as it's always a matter of opinion whether if benefits the\n  overall functionality of the theme.\n\n\n\n\n\n\nSubmission guidelines\n\n\nSubmitting an issue\n\n\nBefore you submit an issue, please search the issue tracker, maybe an issue for\nyour problem already exists and the discussion might inform you of workarounds\nreadily available.\n\n\nWe want to fix all the issues as soon as possible, but before fixing a bug we\nneed to reproduce and confirm it. In order to reproduce bugs we will\nsystematically ask you to provide a minimal reproduction scenario using the\ncustom issue template. Please stick to the issue template.\n\n\nUnfortunately we are not able to investigate / fix bugs without a minimal\nreproduction scenario, so if we don't hear back from you we may close the issue.\n\n\nSubmitting a Pull Request (PR)\n\n\nSearch GitHub for an open or closed PR that relates to your submission. You\ndon't want to duplicate effort. If you do not find a related issue or PR,\ngo ahead.\n\n\n\n\n\n\nDevelopment\n: Fork the project, set up the \ndevelopment environment\n,\n  make your changes in a separate git branch and add descriptive messages to\n  your commits.\n\n\n\n\n\n\nBuild\n: Before submitting a pull requests, build the theme. This is a\n  mandatory requirement for your PR to get accepted, as the theme should at\n  all times be installable through GitHub.\n\n\n\n\n\n\nPull Request\n: After building the theme, commit the compiled output, push\n  your branch to GitHub and send a PR to \nmkdocs-material:master\n. If we\n  suggest changes, make the required updates, rebase your branch and push the\n  changes to your GitHub repository, which will automatically update your PR.\n\n\n\n\n\n\nAfter your PR is merged, you can safely delete your branch and pull the changes\nfrom the main (upstream) repository.", 
            "title": "Contributing"
        }, 
        {
            "location": "/contributing/#contributing", 
            "text": "Interested in contributing to the Material theme? Want to report a bug? Before\nyou do, please read the following guidelines.", 
            "title": "Contributing"
        }, 
        {
            "location": "/contributing/#submission-context", 
            "text": "", 
            "title": "Submission context"
        }, 
        {
            "location": "/contributing/#got-a-question-or-problem", 
            "text": "For quick questions there's no need to open an issue as you can reach us on gitter.im .", 
            "title": "Got a question or problem?"
        }, 
        {
            "location": "/contributing/#found-a-bug", 
            "text": "If you found a bug in the source code, you can help us by submitting an issue\nto the  issue tracker  in our GitHub repository. Even better, you can submit\na Pull Request with a fix. However, before doing so, please read the submission guidelines .", 
            "title": "Found a bug?"
        }, 
        {
            "location": "/contributing/#missing-a-feature", 
            "text": "You can request a new feature by submitting an issue to our GitHub Repository.\nIf you would like to implement a new feature, please submit an issue with a\nproposal for your work first, to be sure that it is of use for everyone, as\nthe Material theme is highly opinionated. Please consider what kind of change\nit is:    For a  major feature , first open an issue and outline your proposal so\n  that it can be discussed. This will also allow us to better coordinate our\n  efforts, prevent duplication of work, and help you to craft the change so\n  that it is successfully accepted into the project.    Small features and bugs  can be crafted and directly submitted as a Pull\n  Request. However, there is no guarantee that your feature will make it into\n  the master, as it's always a matter of opinion whether if benefits the\n  overall functionality of the theme.", 
            "title": "Missing a feature?"
        }, 
        {
            "location": "/contributing/#submission-guidelines", 
            "text": "", 
            "title": "Submission guidelines"
        }, 
        {
            "location": "/contributing/#submitting-an-issue", 
            "text": "Before you submit an issue, please search the issue tracker, maybe an issue for\nyour problem already exists and the discussion might inform you of workarounds\nreadily available.  We want to fix all the issues as soon as possible, but before fixing a bug we\nneed to reproduce and confirm it. In order to reproduce bugs we will\nsystematically ask you to provide a minimal reproduction scenario using the\ncustom issue template. Please stick to the issue template.  Unfortunately we are not able to investigate / fix bugs without a minimal\nreproduction scenario, so if we don't hear back from you we may close the issue.", 
            "title": "Submitting an issue"
        }, 
        {
            "location": "/contributing/#submitting-a-pull-request-pr", 
            "text": "Search GitHub for an open or closed PR that relates to your submission. You\ndon't want to duplicate effort. If you do not find a related issue or PR,\ngo ahead.    Development : Fork the project, set up the  development environment ,\n  make your changes in a separate git branch and add descriptive messages to\n  your commits.    Build : Before submitting a pull requests, build the theme. This is a\n  mandatory requirement for your PR to get accepted, as the theme should at\n  all times be installable through GitHub.    Pull Request : After building the theme, commit the compiled output, push\n  your branch to GitHub and send a PR to  mkdocs-material:master . If we\n  suggest changes, make the required updates, rebase your branch and push the\n  changes to your GitHub repository, which will automatically update your PR.    After your PR is merged, you can safely delete your branch and pull the changes\nfrom the main (upstream) repository.", 
            "title": "Submitting a Pull Request (PR)"
        }, 
        {
            "location": "/license/", 
            "text": "License\n\n\nMIT License\n\n\nCopyright \n 2016 - 2017 Martin Donath\n\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to\ndeal in the Software without restriction, including without limitation the\nrights to use, copy, modify, merge, publish, distribute, sublicense, and/or\nsell copies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\nFROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\nIN THE SOFTWARE.", 
            "title": "License"
        }, 
        {
            "location": "/license/#license", 
            "text": "MIT License  Copyright   2016 - 2017 Martin Donath  Permission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to\ndeal in the Software without restriction, including without limitation the\nrights to use, copy, modify, merge, publish, distribute, sublicense, and/or\nsell copies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\nFROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\nIN THE SOFTWARE.", 
            "title": "License"
        }
    ]
}